Case study based approach, where we see how the solution architecture changes iteratively.

## First Solution Architecture Discussion

What it the time.com 

Starting small
- We don't need database
- We want to start small and accept downtime
- We want to fully scale - vertically and horizontally and remove downtime at a later stage

Let's see how we can proceed. 

### Simple instance that returns time

To start off with, we have a simple T2 instance that tells users what time it is based on the time time zone they are in or the time zone that they select. 

#### We assign an *Elastic IP* to this instance so people can access it.

But as the number of users increase who are accessing this application, the load on the application increases and the instance is no longer able to serve as many requests as possible. 

This is where we will have to vertically scale the instance that we have. ![[Screenshot 2024-04-14 at 10.52.48 PM.png]]
But we will need to have a downtime when we do this! Downtime when we update the instance type from T2 to M5.

#### Time to scale horizontally

Now we are getting tons of users all asking what time it is and now we want to scale horizontally so we add more EC2 instances with different elastic IPs, but our user's need to keep track of each instance's elastic IP to be able to access it, if not they keep hitting the same instance which causes the same problems. 

#### Setting up route 53

So to avoid this hassle, we set up a public hosted DNS server using Route 53 and add an A record with Simple routing policy that returns back all the public IP addresses of the different EC2 instance. 

![[Screenshot 2024-04-14 at 11.03.14 PM.png]]

The problem? When we want to scale up / scale down, we need to manually update the A record 
- Remove the public IP of the instance that was terminated 
- Add the public IP of the instance that was added

![[Screenshot 2024-04-14 at 11.07.19 PM.png]]

But since the route 53 TTL was one hour, they still get the old IP address for the next 1 hour till the TTL expires. 

The users who used the public IP of the terminated EC2 instance now have to wait to query the dns from route 53 again to check available records and pick the next available one. 

#### Scaling Horizontally with a load balancer

We add an elastic load balancer with health checks that distributes traffic across all available instances and we link our load balancer frontend to our DNS on route 53 through an alias record.  This way user's can just access the service through a single url and get the response through the load balancer. 

![[Screenshot 2024-04-14 at 11.12.53 PM.png]]

#### Automating scaling based on usage using an autoscaling group

![[Screenshot 2024-04-14 at 11.11.56 PM.png]]

We can use many different types of policies to mention how we want our autoscaling group to function. It could be schedule based scaling, demand and usage based scaling. 

### EARTHQUAKE !!

Now there was an earthquake and our application went down because the earthquake affected the servers in AZ-1

So to fix this we need to implement multi AZ deployments to ensure our application is highly available. 

So now we have to update our ELB and the AutoScaling group to handle resources across multiple AZs.

This way we have the autoscaling group spin up instances across multiple AZs and also since our ELB is going to distribute traffic across multiple AZs so we reduce application downtime! 

![[Screenshot 2024-04-14 at 11.25.36 PM.png]]

#### Cost Saving 

We know that for sure we need at least one instance must be running in minimum 2 AZs to ensure high availability, so why not we diminish our costs by reserving capacity in both the AZs for a year. We can reserve instances with the minimum capacity for our auto scaling group and for new instance that get spun up, we can either use On-Demand or spot instances for less cost. 

### Summary

In this journey, we have looked at the following: 
1. Public IPs vs Private IPs and EC2 instances - where did these fit in our architecture diagram. 
2. We have also seen the benefit of using an elastic IP vs Route 53 vs Load balancers to handle traffic for our application
3. We also saw Route 53 TTL, A records and when to use Alias Records
4. We have seen how to maintain EC2 instances manually vs Auto Scaling groups
5. Moving to Multi AZ to survive disasters and enabling ELB Health Checks to ensure traffic is routed to only the healthy EC2 instances
6. Setting up Security group rules to ensure instances are only accessible from the ELB
7. Finally to save cost, we reserve capacity when possible

We touched on 5 pillars of a well architected application: cost, performance, reliability, security and operational excellence.

